{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphicalLassoCV Example\n",
    "\n",
    "Last Update: July 22nd, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "from sklearn.covariance import GraphicalLasso\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define true covariance matrix\n",
    "true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n",
    "                     [0.0, 0.4, 0.0, 0.0],\n",
    "                     [0.2, 0.0, 0.3, 0.1],\n",
    "                     [0.0, 0.0, 0.1, 0.7]])\n",
    "\n",
    "# Set seed and generate X from multivaraite norm with specified covariance\n",
    "np.random.seed(0)\n",
    "X = np.random.multivariate_normal(mean=[0, 0, 0, 0], cov=true_cov, size=200)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.51515152,  0.        , -1.06060606,  0.15151515],\n",
       "       [ 0.        ,  2.5       ,  0.        ,  0.        ],\n",
       "       [-1.06060606,  0.        ,  4.24242424, -0.60606061],\n",
       "       [ 0.15151515,  0.        , -0.60606061,  1.51515152]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True precision matrix calculated from the inverse of true covariance matrix\n",
    "true_prec = np.linalg.inv(true_cov)\n",
    "true_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the GraphicalLassoCV model\n",
    "est = GraphicalLassoCV(max_iter = 1000).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.816, 0.051, 0.22 , 0.017],\n",
       "       [0.051, 0.364, 0.018, 0.036],\n",
       "       [0.22 , 0.018, 0.322, 0.094],\n",
       "       [0.017, 0.036, 0.094, 0.69 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The estimated covariance matrix from GraphicalLassoCV\n",
    "np.around(est.covariance_, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.521, -0.17 , -1.063,  0.116],\n",
       "       [-0.17 ,  2.784, -0.   , -0.14 ],\n",
       "       [-1.063, -0.   ,  3.982, -0.518],\n",
       "       [ 0.116, -0.14 , -0.518,  1.524]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The estimated precision matrix from GraphicalLassoCV\n",
    "np.around(est.precision_, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22813021, 0.04914916, 0.02659803, 0.01439404, 0.01058887,\n",
       "       0.00880744, 0.00848886, 0.00818181, 0.00788587, 0.00778962,\n",
       "       0.00760063, 0.00732571, 0.00609326, 0.00506815, 0.0042155 ,\n",
       "       0.0022813 , 0.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of lambdas used in cross validation\n",
    "est.cv_results_[\"alphas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008181811323310086"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The lambda chosen by cross validation\n",
    "est.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The index of the chosen lambda in the list of lambdas\n",
    "ind_lambda = np.where(est.cv_results_[\"alphas\"] == est.alpha_)[0][0]\n",
    "ind_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.505, -4.166, -4.24 , -4.235, -4.238],\n",
       "       [-4.314, -4.045, -4.167, -4.174, -4.165],\n",
       "       [-4.281, -4.037, -4.161, -4.175, -4.171],\n",
       "       [-4.263, -4.033, -4.162, -4.174, -4.178],\n",
       "       [-4.257, -4.03 , -4.163, -4.175, -4.181],\n",
       "       [-4.254, -4.028, -4.163, -4.176, -4.183],\n",
       "       [-4.254, -4.028, -4.164, -4.176, -4.183],\n",
       "       [-4.253, -4.028, -4.164, -4.176, -4.183],\n",
       "       [-4.253, -4.027, -4.164, -4.176, -4.184],\n",
       "       [-4.252, -4.027, -4.164, -4.176, -4.184],\n",
       "       [-4.252, -4.027, -4.165, -4.176, -4.184],\n",
       "       [-4.252, -4.027, -4.165, -4.176, -4.184],\n",
       "       [-4.25 , -4.026, -4.167, -4.178, -4.186],\n",
       "       [-4.248, -4.025, -4.169, -4.18 , -4.187],\n",
       "       [-4.247, -4.025, -4.171, -4.181, -4.188],\n",
       "       [-4.244, -4.023, -4.174, -4.184, -4.19 ],\n",
       "       [-4.395, -4.038, -4.195, -4.209, -4.416]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_scores = np.round(np.hstack((est.cv_results_[\"split0_test_score\"].reshape(-1, 1), \\\n",
    "                       est.cv_results_[\"split1_test_score\"].reshape(-1, 1), \\\n",
    "                        est.cv_results_[\"split2_test_score\"].reshape(-1, 1), \\\n",
    "                        est.cv_results_[\"split3_test_score\"].reshape(-1, 1), \\\n",
    "                        est.cv_results_[\"split4_test_score\"].reshape(-1, 1))\n",
    "                        ), 3)\n",
    "grid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.505, -4.166, -4.24 , -4.235, -4.238],\n",
       "       [-4.314, -4.045, -4.167, -4.174, -4.165],\n",
       "       [-4.281, -4.037, -4.161, -4.175, -4.171],\n",
       "       [-4.263, -4.033, -4.162, -4.174, -4.178],\n",
       "       [-4.257, -4.03 , -4.163, -4.175, -4.181],\n",
       "       [-4.254, -4.028, -4.163, -4.176, -4.183],\n",
       "       [-4.254, -4.028, -4.164, -4.176, -4.183],\n",
       "       [-4.253, -4.028, -4.164, -4.176, -4.183],\n",
       "       [-4.253, -4.027, -4.164, -4.176, -4.184],\n",
       "       [-4.252, -4.027, -4.164, -4.176, -4.184],\n",
       "       [-4.252, -4.027, -4.165, -4.176, -4.184],\n",
       "       [-4.252, -4.027, -4.165, -4.176, -4.184],\n",
       "       [-4.25 , -4.026, -4.167, -4.178, -4.186],\n",
       "       [-4.248, -4.025, -4.169, -4.18 , -4.187],\n",
       "       [-4.247, -4.025, -4.171, -4.181, -4.188],\n",
       "       [-4.244, -4.023, -4.174, -4.184, -4.19 ],\n",
       "       [-4.395, -4.038, -4.195, -4.209, -4.416]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid_scores_ndarray of shape (n_alphas, n_folds): Log-likelihood score on left-out data across folds.\n",
    "np.round(est.grid_scores_, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambdas</th>\n",
       "      <th>score_fold1</th>\n",
       "      <th>score_fold2</th>\n",
       "      <th>score_fold3</th>\n",
       "      <th>score_fold4</th>\n",
       "      <th>score_fold5</th>\n",
       "      <th>Total_score</th>\n",
       "      <th>Average_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.228130</td>\n",
       "      <td>-4.505</td>\n",
       "      <td>-4.166</td>\n",
       "      <td>-4.240</td>\n",
       "      <td>-4.235</td>\n",
       "      <td>-4.238</td>\n",
       "      <td>-21.384</td>\n",
       "      <td>-4.2768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049149</td>\n",
       "      <td>-4.314</td>\n",
       "      <td>-4.045</td>\n",
       "      <td>-4.167</td>\n",
       "      <td>-4.174</td>\n",
       "      <td>-4.165</td>\n",
       "      <td>-20.865</td>\n",
       "      <td>-4.1730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026598</td>\n",
       "      <td>-4.281</td>\n",
       "      <td>-4.037</td>\n",
       "      <td>-4.161</td>\n",
       "      <td>-4.175</td>\n",
       "      <td>-4.171</td>\n",
       "      <td>-20.825</td>\n",
       "      <td>-4.1650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014394</td>\n",
       "      <td>-4.263</td>\n",
       "      <td>-4.033</td>\n",
       "      <td>-4.162</td>\n",
       "      <td>-4.174</td>\n",
       "      <td>-4.178</td>\n",
       "      <td>-20.810</td>\n",
       "      <td>-4.1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010589</td>\n",
       "      <td>-4.257</td>\n",
       "      <td>-4.030</td>\n",
       "      <td>-4.163</td>\n",
       "      <td>-4.175</td>\n",
       "      <td>-4.181</td>\n",
       "      <td>-20.806</td>\n",
       "      <td>-4.1612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008807</td>\n",
       "      <td>-4.254</td>\n",
       "      <td>-4.028</td>\n",
       "      <td>-4.163</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.183</td>\n",
       "      <td>-20.804</td>\n",
       "      <td>-4.1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.008489</td>\n",
       "      <td>-4.254</td>\n",
       "      <td>-4.028</td>\n",
       "      <td>-4.164</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.183</td>\n",
       "      <td>-20.805</td>\n",
       "      <td>-4.1610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.008182</td>\n",
       "      <td>-4.253</td>\n",
       "      <td>-4.028</td>\n",
       "      <td>-4.164</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.183</td>\n",
       "      <td>-20.804</td>\n",
       "      <td>-4.1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007886</td>\n",
       "      <td>-4.253</td>\n",
       "      <td>-4.027</td>\n",
       "      <td>-4.164</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.184</td>\n",
       "      <td>-20.804</td>\n",
       "      <td>-4.1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007790</td>\n",
       "      <td>-4.252</td>\n",
       "      <td>-4.027</td>\n",
       "      <td>-4.164</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.184</td>\n",
       "      <td>-20.803</td>\n",
       "      <td>-4.1606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.007601</td>\n",
       "      <td>-4.252</td>\n",
       "      <td>-4.027</td>\n",
       "      <td>-4.165</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.184</td>\n",
       "      <td>-20.804</td>\n",
       "      <td>-4.1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.007326</td>\n",
       "      <td>-4.252</td>\n",
       "      <td>-4.027</td>\n",
       "      <td>-4.165</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.184</td>\n",
       "      <td>-20.804</td>\n",
       "      <td>-4.1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006093</td>\n",
       "      <td>-4.250</td>\n",
       "      <td>-4.026</td>\n",
       "      <td>-4.167</td>\n",
       "      <td>-4.178</td>\n",
       "      <td>-4.186</td>\n",
       "      <td>-20.807</td>\n",
       "      <td>-4.1614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.005068</td>\n",
       "      <td>-4.248</td>\n",
       "      <td>-4.025</td>\n",
       "      <td>-4.169</td>\n",
       "      <td>-4.180</td>\n",
       "      <td>-4.187</td>\n",
       "      <td>-20.809</td>\n",
       "      <td>-4.1618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.004216</td>\n",
       "      <td>-4.247</td>\n",
       "      <td>-4.025</td>\n",
       "      <td>-4.171</td>\n",
       "      <td>-4.181</td>\n",
       "      <td>-4.188</td>\n",
       "      <td>-20.812</td>\n",
       "      <td>-4.1624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002281</td>\n",
       "      <td>-4.244</td>\n",
       "      <td>-4.023</td>\n",
       "      <td>-4.174</td>\n",
       "      <td>-4.184</td>\n",
       "      <td>-4.190</td>\n",
       "      <td>-20.815</td>\n",
       "      <td>-4.1630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.395</td>\n",
       "      <td>-4.038</td>\n",
       "      <td>-4.195</td>\n",
       "      <td>-4.209</td>\n",
       "      <td>-4.416</td>\n",
       "      <td>-21.253</td>\n",
       "      <td>-4.2506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lambdas  score_fold1  score_fold2  score_fold3  score_fold4  score_fold5  \\\n",
       "0   0.228130       -4.505       -4.166       -4.240       -4.235       -4.238   \n",
       "1   0.049149       -4.314       -4.045       -4.167       -4.174       -4.165   \n",
       "2   0.026598       -4.281       -4.037       -4.161       -4.175       -4.171   \n",
       "3   0.014394       -4.263       -4.033       -4.162       -4.174       -4.178   \n",
       "4   0.010589       -4.257       -4.030       -4.163       -4.175       -4.181   \n",
       "5   0.008807       -4.254       -4.028       -4.163       -4.176       -4.183   \n",
       "6   0.008489       -4.254       -4.028       -4.164       -4.176       -4.183   \n",
       "7   0.008182       -4.253       -4.028       -4.164       -4.176       -4.183   \n",
       "8   0.007886       -4.253       -4.027       -4.164       -4.176       -4.184   \n",
       "9   0.007790       -4.252       -4.027       -4.164       -4.176       -4.184   \n",
       "10  0.007601       -4.252       -4.027       -4.165       -4.176       -4.184   \n",
       "11  0.007326       -4.252       -4.027       -4.165       -4.176       -4.184   \n",
       "12  0.006093       -4.250       -4.026       -4.167       -4.178       -4.186   \n",
       "13  0.005068       -4.248       -4.025       -4.169       -4.180       -4.187   \n",
       "14  0.004216       -4.247       -4.025       -4.171       -4.181       -4.188   \n",
       "15  0.002281       -4.244       -4.023       -4.174       -4.184       -4.190   \n",
       "16  0.000000       -4.395       -4.038       -4.195       -4.209       -4.416   \n",
       "\n",
       "    Total_score  Average_score  \n",
       "0       -21.384        -4.2768  \n",
       "1       -20.865        -4.1730  \n",
       "2       -20.825        -4.1650  \n",
       "3       -20.810        -4.1620  \n",
       "4       -20.806        -4.1612  \n",
       "5       -20.804        -4.1608  \n",
       "6       -20.805        -4.1610  \n",
       "7       -20.804        -4.1608  \n",
       "8       -20.804        -4.1608  \n",
       "9       -20.803        -4.1606  \n",
       "10      -20.804        -4.1608  \n",
       "11      -20.804        -4.1608  \n",
       "12      -20.807        -4.1614  \n",
       "13      -20.809        -4.1618  \n",
       "14      -20.812        -4.1624  \n",
       "15      -20.815        -4.1630  \n",
       "16      -21.253        -4.2506  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary information \n",
    "alphas = np.array(est.cv_results_[\"alphas\"]).reshape(len(est.cv_results_[\"alphas\"]),1)\n",
    "df_temp = pd.DataFrame(np.concatenate((alphas,grid_scores),axis = 1),\n",
    "                       columns = ['lambdas','score_fold1','score_fold2','score_fold3','score_fold4','score_fold5'])\n",
    "df_temp['Total_score'] = grid_scores.sum(axis = 1)\n",
    "df_temp['Average_score'] = grid_scores.sum(axis = 1)/grid_scores.shape[1]\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lambda =  0.22813020648178528\n",
      "Number of zero entries in the precision matrix is :  12\n",
      "For lambda =  0.049149163068849484\n",
      "Number of zero entries in the precision matrix is :  6\n",
      "For lambda =  0.026598029308124216\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.014394042927746542\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.010588866190156324\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.008807435274025993\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.008488861740795791\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.008181811323310086\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.007885867219221507\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.007789617396298071\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.00760062770233054\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.007325705577266546\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.006093256496935097\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.005068150002186586\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.0042155035583327245\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.002281302064817854\n",
      "Number of zero entries in the precision matrix is :  2\n",
      "For lambda =  0.0\n",
      "Number of zero entries in the precision matrix is :  0\n"
     ]
    }
   ],
   "source": [
    "# Find the precision matrix and number of zero entries in the matrix for each lambda\n",
    "prec_mx_list = []\n",
    "num_zeros_list = []\n",
    "\n",
    "for i in range(len(est.cv_results_[\"alphas\"])):\n",
    "    est_lambda = GraphicalLasso(alpha = est.cv_results_[\"alphas\"][i], max_iter = 1000).fit(X)\n",
    "    prec_mx_list.append(est_lambda.precision_)\n",
    "    \n",
    "    non_zero = (np.abs(est_lambda.precision_) > 0.02)\n",
    "    num_zeros = non_zero.shape[0] * non_zero.shape[1] - np.sum(non_zero*1)\n",
    "    num_zeros_list.append(num_zeros)\n",
    "    \n",
    "    print('For lambda = ', est.cv_results_[\"alphas\"][i])\n",
    "    print('Number of zero entries in the precision matrix is : ', num_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambdas</th>\n",
       "      <th>score_fold1</th>\n",
       "      <th>score_fold2</th>\n",
       "      <th>score_fold3</th>\n",
       "      <th>score_fold4</th>\n",
       "      <th>score_fold5</th>\n",
       "      <th>Total_score</th>\n",
       "      <th>Average_score</th>\n",
       "      <th>Num_Zeros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.228130</td>\n",
       "      <td>-4.505</td>\n",
       "      <td>-4.166</td>\n",
       "      <td>-4.240</td>\n",
       "      <td>-4.235</td>\n",
       "      <td>-4.238</td>\n",
       "      <td>-21.384</td>\n",
       "      <td>-4.2768</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049149</td>\n",
       "      <td>-4.314</td>\n",
       "      <td>-4.045</td>\n",
       "      <td>-4.167</td>\n",
       "      <td>-4.174</td>\n",
       "      <td>-4.165</td>\n",
       "      <td>-20.865</td>\n",
       "      <td>-4.1730</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026598</td>\n",
       "      <td>-4.281</td>\n",
       "      <td>-4.037</td>\n",
       "      <td>-4.161</td>\n",
       "      <td>-4.175</td>\n",
       "      <td>-4.171</td>\n",
       "      <td>-20.825</td>\n",
       "      <td>-4.1650</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014394</td>\n",
       "      <td>-4.263</td>\n",
       "      <td>-4.033</td>\n",
       "      <td>-4.162</td>\n",
       "      <td>-4.174</td>\n",
       "      <td>-4.178</td>\n",
       "      <td>-20.810</td>\n",
       "      <td>-4.1620</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010589</td>\n",
       "      <td>-4.257</td>\n",
       "      <td>-4.030</td>\n",
       "      <td>-4.163</td>\n",
       "      <td>-4.175</td>\n",
       "      <td>-4.181</td>\n",
       "      <td>-20.806</td>\n",
       "      <td>-4.1612</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008807</td>\n",
       "      <td>-4.254</td>\n",
       "      <td>-4.028</td>\n",
       "      <td>-4.163</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.183</td>\n",
       "      <td>-20.804</td>\n",
       "      <td>-4.1608</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.008489</td>\n",
       "      <td>-4.254</td>\n",
       "      <td>-4.028</td>\n",
       "      <td>-4.164</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.183</td>\n",
       "      <td>-20.805</td>\n",
       "      <td>-4.1610</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.008182</td>\n",
       "      <td>-4.253</td>\n",
       "      <td>-4.028</td>\n",
       "      <td>-4.164</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.183</td>\n",
       "      <td>-20.804</td>\n",
       "      <td>-4.1608</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007886</td>\n",
       "      <td>-4.253</td>\n",
       "      <td>-4.027</td>\n",
       "      <td>-4.164</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.184</td>\n",
       "      <td>-20.804</td>\n",
       "      <td>-4.1608</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007790</td>\n",
       "      <td>-4.252</td>\n",
       "      <td>-4.027</td>\n",
       "      <td>-4.164</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.184</td>\n",
       "      <td>-20.803</td>\n",
       "      <td>-4.1606</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.007601</td>\n",
       "      <td>-4.252</td>\n",
       "      <td>-4.027</td>\n",
       "      <td>-4.165</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.184</td>\n",
       "      <td>-20.804</td>\n",
       "      <td>-4.1608</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.007326</td>\n",
       "      <td>-4.252</td>\n",
       "      <td>-4.027</td>\n",
       "      <td>-4.165</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>-4.184</td>\n",
       "      <td>-20.804</td>\n",
       "      <td>-4.1608</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006093</td>\n",
       "      <td>-4.250</td>\n",
       "      <td>-4.026</td>\n",
       "      <td>-4.167</td>\n",
       "      <td>-4.178</td>\n",
       "      <td>-4.186</td>\n",
       "      <td>-20.807</td>\n",
       "      <td>-4.1614</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.005068</td>\n",
       "      <td>-4.248</td>\n",
       "      <td>-4.025</td>\n",
       "      <td>-4.169</td>\n",
       "      <td>-4.180</td>\n",
       "      <td>-4.187</td>\n",
       "      <td>-20.809</td>\n",
       "      <td>-4.1618</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.004216</td>\n",
       "      <td>-4.247</td>\n",
       "      <td>-4.025</td>\n",
       "      <td>-4.171</td>\n",
       "      <td>-4.181</td>\n",
       "      <td>-4.188</td>\n",
       "      <td>-20.812</td>\n",
       "      <td>-4.1624</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002281</td>\n",
       "      <td>-4.244</td>\n",
       "      <td>-4.023</td>\n",
       "      <td>-4.174</td>\n",
       "      <td>-4.184</td>\n",
       "      <td>-4.190</td>\n",
       "      <td>-20.815</td>\n",
       "      <td>-4.1630</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.395</td>\n",
       "      <td>-4.038</td>\n",
       "      <td>-4.195</td>\n",
       "      <td>-4.209</td>\n",
       "      <td>-4.416</td>\n",
       "      <td>-21.253</td>\n",
       "      <td>-4.2506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lambdas  score_fold1  score_fold2  score_fold3  score_fold4  score_fold5  \\\n",
       "0   0.228130       -4.505       -4.166       -4.240       -4.235       -4.238   \n",
       "1   0.049149       -4.314       -4.045       -4.167       -4.174       -4.165   \n",
       "2   0.026598       -4.281       -4.037       -4.161       -4.175       -4.171   \n",
       "3   0.014394       -4.263       -4.033       -4.162       -4.174       -4.178   \n",
       "4   0.010589       -4.257       -4.030       -4.163       -4.175       -4.181   \n",
       "5   0.008807       -4.254       -4.028       -4.163       -4.176       -4.183   \n",
       "6   0.008489       -4.254       -4.028       -4.164       -4.176       -4.183   \n",
       "7   0.008182       -4.253       -4.028       -4.164       -4.176       -4.183   \n",
       "8   0.007886       -4.253       -4.027       -4.164       -4.176       -4.184   \n",
       "9   0.007790       -4.252       -4.027       -4.164       -4.176       -4.184   \n",
       "10  0.007601       -4.252       -4.027       -4.165       -4.176       -4.184   \n",
       "11  0.007326       -4.252       -4.027       -4.165       -4.176       -4.184   \n",
       "12  0.006093       -4.250       -4.026       -4.167       -4.178       -4.186   \n",
       "13  0.005068       -4.248       -4.025       -4.169       -4.180       -4.187   \n",
       "14  0.004216       -4.247       -4.025       -4.171       -4.181       -4.188   \n",
       "15  0.002281       -4.244       -4.023       -4.174       -4.184       -4.190   \n",
       "16  0.000000       -4.395       -4.038       -4.195       -4.209       -4.416   \n",
       "\n",
       "    Total_score  Average_score  Num_Zeros  \n",
       "0       -21.384        -4.2768         12  \n",
       "1       -20.865        -4.1730          6  \n",
       "2       -20.825        -4.1650          2  \n",
       "3       -20.810        -4.1620          2  \n",
       "4       -20.806        -4.1612          2  \n",
       "5       -20.804        -4.1608          2  \n",
       "6       -20.805        -4.1610          2  \n",
       "7       -20.804        -4.1608          2  \n",
       "8       -20.804        -4.1608          2  \n",
       "9       -20.803        -4.1606          2  \n",
       "10      -20.804        -4.1608          2  \n",
       "11      -20.804        -4.1608          2  \n",
       "12      -20.807        -4.1614          2  \n",
       "13      -20.809        -4.1618          2  \n",
       "14      -20.812        -4.1624          2  \n",
       "15      -20.815        -4.1630          2  \n",
       "16      -21.253        -4.2506          0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the number of zeros information to the summary information dataframe\n",
    "df_temp['Num_Zeros'] = np.array(num_zeros_list)\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.22596272, -0.        , -0.        , -0.        ],\n",
       "       [-0.        ,  2.74605582, -0.        , -0.        ],\n",
       "       [-0.        , -0.        ,  3.10477899, -0.        ],\n",
       "       [-0.        , -0.        , -0.        ,  1.44860969]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimated precision matrix for the first lambda\n",
    "prec_mx_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.39660482, -0.03306993, -0.7758674 ,  0.        ],\n",
       "       [-0.03306993,  2.74694758, -0.        , -0.        ],\n",
       "       [-0.7758674 , -0.        ,  3.57628844, -0.24337161],\n",
       "       [ 0.        , -0.        , -0.24337161,  1.46744183]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimated precision matrix for the second lambda\n",
    "prec_mx_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.52069737, -0.17033761, -1.06283639,  0.11603294],\n",
       "       [-0.17033761,  2.78366128, -0.        , -0.14048774],\n",
       "       [-1.06283639, -0.        ,  3.98232618, -0.51782878],\n",
       "       [ 0.11603294, -0.14048774, -0.51782878,  1.52380577]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimated precision matrix for the chosen lambda\n",
    "prec_mx_list[ind_lambda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
